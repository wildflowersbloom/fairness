# fairness
Data sheets for datasets. 
1. For what purpose was the data set created?
Classify as good or bad credit risk, false positives being worse, losing the bank money.
2. Who created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?
The data was uploaded by Prof. Dr. Hans Hofmann, Institut fur Statistik und Okonomie, Universitat Hamburg
from the years 1973 to 1975 for a large regional bank in southern Germany. The original generator of the data is unknown beyond this.
3. Who funded the creation of the dataset?
The bank?
4. Any other comments?
The information above was mainly taken from the following source: http://www1.beuth-hochschule.de/FB_II/reports/Report-2019-004.pdf which provides a story for the data tracing it back to academic work from as early as 1981.
Bad credits are heavily oversampled (prevalence 5%)
All debtors must have passed some checks of creditworthiness before being granted the credit.
5. What do the instances that comprise the dataset represent?
People
6. How many instances are there in total?
1000 total, 300 bad ones and 700 good ones
7. Does the dataset contain all possible instances or is it a sample
(not necessarily random) of instances from a larger set?
Must be a sample
8. What data does each instance consist of? 
9. Is there a label or target associated with each instance?
10. Is any information missing
from individual instances?
11. Are relationships between individual instances made explicit (for
example, users’ movie ratings, social
network links)?
12. Are there recommended data
splits (for example, training, development/validation, testing)?
13. Are there any errors, sources of
noise, or redundancies in the dataset?
14. Is the dataset self-contained,
or does it link to or otherwise rely
on external resources (for example,
websites, tweets, other datasets)? 
15. Does the dataset contain data
that might be considered confidential
(for example, data that is protected by
legal privilege or by doctor–patient
confidentiality, data that includes the
content of individuals’ non-public
communications)?
16. Does the dataset contain data
that, if viewed directly, might be offensive, insulting, threatening, or
might otherwise cause anxiety?No
17. Does the dataset identify any subpopulations (for example, by age, gender)? 
18. Is it possible to identify individuals (that is, one or more natural
persons), either directly or indirectly
(that is, in combination with other
data) from the dataset?
19. Does the dataset contain data
that might be considered sensitive
in any way (for example, data that reveals race or ethnic origins, sexual
orientations, religious beliefs, political opinions or union memberships,
or locations; financial or health data;
biometric or genetic data; forms of
government identification, such as social security numbers; criminal history)?
20. Any other comments?
Collection process. As with the
questions in the previous section,
dataset creators should read through
these questions prior to any data collection to flag potential issues and
then provide answers once collection
is complete. In addition to the goals
outlined earlier, the following questions are designed to elicit information that may help researchers and
practitioners to create alternative
datasets with similar characteristics. Again, questions that apply only
to datasets that relate to people are
grouped together at the end of the
section.
21. How was the data associated
with each instance acquired? Was
the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/
derived from other data (for example,
part-of-speech tags, model-based
guesses for age or language)? If the
data was reported by subjects or indirectly inferred/derived from other
data, was the data validated/verified?
If so, please describe how.
22. What mechanisms or procedures were used to collect the data
(for example, hardware apparatuses
or sensors, manual human curation,
software programs, software APIs)?
How were these mechanisms or procedures validated?
23. If the dataset is a sample from
a larger set, what was the sampling
strategy (for example, deterministic,
probabilistic with specific sampling
probabilities)? Probably
24. Who was involved in the data
collection process (for example, students, crowdworkers, contractors)
and how were they compensated (for
example, how much were crowdworkers paid)?
25. Over what timeframe was the
data collected?
26. Were any ethical review processes conducted (for example, by an institutional review board)?
27. Did you collect the data from
the individuals in question directly,
or obtain it via third parties or other
sources (for example, websites)? Indirectly 
28. Were the individuals in question notified about the data collection? Most likely not explicity 
29. Did the individuals in question
consent to the collection and use of
their data? Most likely not explicitly
the individuals consented.
30. If consent was obtained, were the consenting individuals provided
with a mechanism to revoke their consent in the future or for certain uses? 

